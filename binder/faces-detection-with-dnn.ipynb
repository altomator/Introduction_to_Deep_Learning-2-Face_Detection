{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This Python 3 notebook extracts images of a Gallica document (using the IIIF protocol), and then applies face detection to the images**\n",
    "1. Extract the document technical image metadata from its IIIF manifest\n",
    "2. Load the IIIF images\n",
    "3. Apply a SSD Resnet model with openCV/dnn module and perform a (basic) evaluation using the IOU method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert here the Gallica document ID you want to process\n",
    "docID = '12148/bpt6k4604559t' # quotidien\n",
    "#docID = '12148/btv1b6931954n' # photo\n",
    "#docID = '12148/btv1b10336854c' # album\n",
    "#docID = '12148/btv1b10544068q' # estampe\n",
    "#docID = '12148/bpt6k65414058' # Vogue magazine\n",
    "\n",
    "# do we want to evaluate the detection against a ground truth?\n",
    "eval_flag = True\n",
    "# IIIF export factor (%)\n",
    "doc_export_factor = 10\n",
    "# get docMax images\n",
    "doc_max = 1\n",
    "# CSV data export\n",
    "output = \"OUT_csv\"\n",
    "# minimum confidence score to keep the detections\n",
    "min_confidence = 0.15\n",
    "# detecting  homothetic contours\n",
    "homothetic_threshold = 1.3 # 30%  tolerance\n",
    "area_threshold =1.4 # 40%  tolerance\n",
    "\n",
    "# Evaluation of performances\n",
    "# define the `Detection` object\n",
    "from collections import namedtuple\n",
    "Detection = namedtuple(\"Detection\", [\"image_path\", \"gt\", \"pred\"])\n",
    "evals = []\n",
    "gt_file = \"GT_10%.csv\"\n",
    "gt = {} # dictionary for the GT crops\n",
    "iou_mean=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We ask for the document IIIF manifest to know more about its images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we build the IIIF URL\n",
    "import requests\n",
    "\n",
    "METADATA_BASEURL = 'https://gallica.bnf.fr/iiif/ark:/'\n",
    "req_url = \"\".join([METADATA_BASEURL, docID, '/manifest.json'])\n",
    "print (req_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we ask for the IIIF manifest. The call returns a dictionary\n",
    "r = requests.get(req_url)\n",
    "r.raise_for_status()\n",
    "json_4img = r.json()\n",
    "print (json_4img.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now we load the images files thanks to the IIIF Image protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iiif_api import IIIF #  get the image files with the IIIF Image API (PyGallica package again)\n",
    "\n",
    "# get the sequence of images metadata. It's a list\n",
    "sequences = json_4img.get('sequences')\n",
    "# get the canvases, first element of the list. Its a dict\n",
    "canvases = sequences[0]\n",
    "print (canvases.keys())\n",
    "# parse each canvas data for each image\n",
    "# each canvas has these keys: [u'height', u'width', u'@type', u'images', u'label', u'@id', u'thumbnail']\n",
    "n_images = 0\n",
    "urlsIIIF = []\n",
    "print (\"... getting image metadata from the IIIF manifest\")\n",
    "for c in canvases.get('canvases'): \n",
    "    n_images += 1\n",
    "    print (\" label:\",c.get('label'),\" width:\",c.get('width'), \" height:\",c.get('height'))\n",
    "    # we also get a Gallica thumbnail (it's not a IIIF image)\n",
    "    thumbnail = c.get('thumbnail')\n",
    "    urlThumbnail = thumbnail.get('@id')\n",
    "    #print \" thumbnail: \",urlThumbnail  \n",
    "    # we build the IIIF URL. We ask for the full image with a size factor of docExportFactor\n",
    "    urlIIIF = \"\".join([docID,'/f',str(n_images)]), 'full', \"\".join(['pct:',str(doc_export_factor)]), '0', 'native', 'jpg'\n",
    "    urlsIIIF.append(urlIIIF)\n",
    "    #IIIF.iiif()\n",
    "    if n_images >= doc_max:\n",
    "        break\n",
    "     \n",
    "print (\"-------\")\n",
    "print (f\"... we get {doc_max} images on {len(canvases.get('canvases'))}\\n\")\n",
    "print (\"... now downloading the images\")\n",
    "foo=[IIIF.iiif(u[0],u[1],u[2],u[3],u[4],u[5]) for u in urlsIIIF]\n",
    "print (\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We display a tabular view of the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image # pip install pillow\n",
    "from PIL.Image import Image as PilImage\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def path_to_pil(file):   \n",
    "    fileName = \"\".join([docID,\"/\",file]) # the images have been stored in a folder based on the document ID like 12148/btv1b103365619\n",
    "    #print \"--- loading image \",fileName,\"...\"\n",
    "    img = Image.open(fileName)\n",
    "    return img\n",
    "\n",
    "def display_images(\n",
    "    images, \n",
    "    columns=6, width=18, height=8, max_images=20, \n",
    "    label_wrap_length=20, label_font_size=8):\n",
    "\n",
    "    if len(images) == 1:\n",
    "        display(images[0])\n",
    "        return \n",
    "    if not images:\n",
    "        print (\"No images to display!\")\n",
    "        return\n",
    "\n",
    "    if len(images) > max_images:\n",
    "        print (\"Showing\", max_images, \"images of\", len(images))\n",
    "        images=images[0:max_images]\n",
    "\n",
    "    height = max(height, int(len(images)/columns) * height)\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        plt.subplot(int(len(images) / columns) + 1, columns, i + 1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        if hasattr(image, 'filename'):\n",
    "            title=image.filename\n",
    "            if title.endswith(\"/\"): title = title[0:-1]\n",
    "            title=os.path.basename(title)\n",
    "            title=textwrap.wrap(title, label_wrap_length)\n",
    "            title=\"\\n\".join(title)\n",
    "            plt.title(title, fontsize=label_font_size); \n",
    "            \n",
    "# first we read the images\n",
    "import os, fnmatch\n",
    "entries = fnmatch.filter(os.listdir(docID), '*.jpg')\n",
    "images = [path_to_pil(e) for e in entries]\n",
    "display_images(images)\n",
    "#for im in images:\n",
    "#   display(im)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now we process the images for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# using the dnn module : https://docs.opencv.org/master/d2/d58/tutorial_table_of_content_dnn.html\n",
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import csv\n",
    "\n",
    "#######################\n",
    "output_dir = os.path.realpath(output)\n",
    "if not os.path.isdir(output_dir):\n",
    "    print(f\"\\n  Output .csv directory {output} does not exist!\\n\")\n",
    "    os.mkdir(output_dir);\n",
    "else:\n",
    "    print (f\"\\n... CSV files will be saved to {output}\\n\")\n",
    "\n",
    "# writing the results in out_path\n",
    "out_path = os.path.join(output, \"classifications.csv\" )\n",
    "out_file = open(out_path,\"w\")\n",
    "\n",
    "# detecting  homothetic contours\n",
    "homothetic_threshold = 1.3 # 30%  tolerance\n",
    "area_threshold =1.4 # 40%  tolerance\n",
    "n_faces = 0\n",
    "\n",
    "\n",
    "# test if the bounding box is homothetic to the source image and has a similar size\n",
    "def homothetic(c1,c2,area1,area2):  # (x,y,w,h)\n",
    "    ratio1 = float(c1[2]) / float(c1[3])\n",
    "    ratio2 = float(c2[2]) / float(c2[3])\n",
    "    tmp=max(ratio1,ratio2)/min(ratio1,ratio2)\n",
    "    print (\"ratio area:%f\" % (area1/area2))\n",
    "    print (\"ratio w: %f - ratio h : %f - max-min : %f\" % (ratio1, ratio2, tmp))\n",
    "    if tmp < homothetic_threshold and ((area1/area2) < area_threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def process_image(im):\n",
    "    \n",
    "\tglobal n_faces # total number of detected faces\n",
    "\tn_faces_im = 0 #  number of detected faces\n",
    "    \n",
    "\t# load the input image and construct an input blob for the image\n",
    "\t# by resizing to a fixed 300x300 pixels and then normalizing it\n",
    "\tfile_name=im.filename\n",
    "\timage = cv2.imread(file_name)\n",
    "\t(h, w) = image.shape[:2]\n",
    "\tareaImg = h * w\n",
    "\tblob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
    "\toutText=\"\"\n",
    "\tprint (\"\\n**************\\n\",file_name)\n",
    "\tdocID = file_name[0:-4]\n",
    "    \n",
    "\t# pass the blob through the network and obtain the detections and predictions\n",
    "\tnet.setInput(blob)\n",
    "\tdetections = net.forward()\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t\t# extract the confidence (i.e., probability) associated with the prediction\n",
    "\t\t\tconfidence = detections[0, 0, i, 2]\n",
    "\t\t\t# filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "\t\t\tif (confidence > min_confidence):\n",
    "\t\t\t\t# compute the (x, y)-coordinates of the bounding box for the object\n",
    "\t\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t\t\twBox = endX-startX\n",
    "\t\t\t\thBox = endY-startY\n",
    "\t\t\t\tif ((endX > w) or (endY > h)):\n",
    "\t\t\t\t\tprint (f\" # out of image : {w} {h} #\")\n",
    "\t\t\t\telif homothetic((0,0,w,h),(startX, startY, wBox, hBox), areaImg, wBox*hBox):\n",
    "\t\t\t\t\tprint (f\" # homothetic : {w} {h} #\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tn_faces += 1\n",
    "\t\t\t\t\ttext = \"{:.2f}%\".format(confidence * 100)\n",
    "\t\t\t\t\t#print \"\\t%s\" % text\n",
    "\t\t\t\t\t#print (startX, startY,(endX-startX),(endY-startY))\n",
    "\t\t\t\t\t# draw the boxes. If evaluation, later\n",
    "\t\t\t\t\tif not(eval_flag):\n",
    "\t\t\t\t\t\tcv2.rectangle(image, (startX, startY), (endX, endY),(0, 0, 255), 1)\n",
    "\t\t\t\t\t\tcv2.putText(image, file_name, (startX, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 00), 1)\n",
    "\t\t\t\t\t\tcv2.putText(image, text, (startX, startY+30),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 200),2)\n",
    "\t\t\t\t\t# build the csv data\n",
    "\t\t\t\t\tif (outText ==\"\"):\n",
    "\t\t\t\t\t\t# for evaluation, we only consider the case of one face per image\n",
    "\t\t\t\t\t\tif eval_flag:\n",
    "\t\t\t\t\t\t\tif file_name in gt.keys():\n",
    "\t\t\t\t\t\t\t\tprint (\" -> we have a GT\")\n",
    "\t\t\t\t\t\t\t\tgt_crop = gt[file_name]\n",
    "\t\t\t\t\t\t\t\tevals.append(Detection(file_name,gt_crop,[startX, startY, endX, endY]))\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tprint (f\" -> {file_name} has no GT\")\n",
    "\t\t\t\t\t\toutText = \"%d,%d,%d,%d,%.2f\" % (startX, startY,wBox,hBox, confidence)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\toutText = \"%s %d,%d,%d,%d,%.2f\" % (outText, startX, startY,wBox,hBox,confidence)\n",
    "\n",
    "    # convert from openCV2 to PIL. Notice the COLOR_BGR2RGB which means that \n",
    "\t# the color is converted from BGR to RGB\n",
    "\tcolor_coverted = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tpil_image=Image.fromarray(color_coverted)\n",
    "        \n",
    "\tif outText != \"\":\n",
    "\t\tprint (\"->\",outText)\n",
    "\t\t# write in the .csv file\n",
    "\t\tprint (\"%s\\t%s\" % (file_name,outText), file=out_file) \n",
    "\telse:\n",
    "\t\tprint (\" --> no detection!\")\n",
    "\t\tif eval_flag:\n",
    "\t\t\tif file_name in gt.keys():\n",
    "\t\t\t\tprint (\" ...but we have a GT!\")\n",
    "\t\t\t\tgt_crop = gt[file_name]\n",
    "\t\t\t\tevals.append(Detection(file_name,gt_crop,[0,0,0,0]))\n",
    "\t# show the output image\n",
    "\tif not(eval_flag):\n",
    "\t\tdisplay(pil_image)\n",
    "        \n",
    "##############################\n",
    "# Evaluation\n",
    "# IOU computation\n",
    "def intersection_over_union(boxA, boxB):\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(boxA[0], boxB[0])\n",
    "\tyA = max(boxA[1], boxB[1])\n",
    "\txB = min(boxA[2], boxB[2])\n",
    "\tyB = min(boxA[3], boxB[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou\n",
    "\n",
    "# building the image with its GT and prediction crops\n",
    "def process_eval(detection):\n",
    "\tprint (detection)\n",
    "\t# load the image\n",
    "\timage = cv2.imread(detection.image_path)\n",
    "\t# draw the ground-truth bounding box along with the predicted\n",
    "\t# bounding box\n",
    "\tcv2.rectangle(image, tuple(detection.gt[:2]),\n",
    "\t\ttuple(detection.gt[2:]), (0, 255, 0), 1)\n",
    "\tcv2.rectangle(image, tuple(detection.pred[:2]),\n",
    "\t\ttuple(detection.pred[2:]), (0, 0, 200), 1)\n",
    "\t# compute the intersection over union and display it\n",
    "\tiou = intersection_over_union(detection.gt, detection.pred)\n",
    "\tcv2.putText(image, detection.image_path, (10, 30),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
    "\tcv2.putText(image, \"IoU: {:.4f}\".format(iou), (10, 50),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 200, 0), 2)\n",
    "\tprint(\" -> {}: {:.3f}\".format(detection.image_path, iou))\n",
    "\tiou_mean.append(iou)\n",
    "\t# show the output image\n",
    "\tcolor_coverted = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tpil_image=Image.fromarray(color_coverted)\n",
    "\t#cv2.imshow(\"Image\", image)\n",
    "\t#cv2.waitKey(0)\n",
    "\tdisplay(pil_image)\n",
    "    \n",
    "    \n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "## Main ##       \n",
    "print(\" loading model...\")\n",
    "# Single Shot Detector (SSD) model / https://arxiv.org/abs/1512.02325\n",
    "# https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11\n",
    "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "# reading the ground truth\n",
    "if eval_flag:\n",
    "\tprint(\"... reading the ground truth\")\n",
    "\twith open(gt_file, newline='') as csvfile:\n",
    "\t\tspamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "\t\tfor row in spamreader:\n",
    "\t\t\t#print(', '.join(row))\n",
    "\t\t\tprint (\"  file: \",row[0]) # the file\n",
    "\t\t\tcrop = row[1].split(',') # the crop\n",
    "\t\t\tint_array = [int(numeric_string) for numeric_string in crop]\n",
    "\t\t\tgt[row[0]] = int_array\n",
    "\tprint (f\"... we get {len(gt)} images with a ground truth\")\n",
    "\n",
    "print (\"... now infering\")\n",
    "foo=[process_image(im) for im in images]\n",
    "out_file.close()\n",
    "print (f\"\\n ... writing classification data in {output} \\n\")\n",
    "\n",
    "print (f\"\\n ### faces detected: {n_faces} ###\")\n",
    "print (f\" ### images analysed: {len(images)} ###\")\n",
    "\n",
    "if eval_flag:\n",
    "\tprint(\"\\n... now evaluating\")\n",
    "\t# processing the predictions which have a GT\n",
    "\tif len(evals)==0:\n",
    "\t\tprint (\" -> we don't have any GT!\")\n",
    "\telse:\n",
    "\t\t[process_eval(e) for e in evals]\n",
    "\t\tprint (\"---------------------------\")\n",
    "\t\tprint (\" -> average of IOUs: {:.3f}\".format(Average(iou_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
