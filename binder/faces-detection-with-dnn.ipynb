{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This Python 3 notebook extracts images of Gallica documents (using the IIIF protocol), and then applies face detection to the images**\n",
    "1. Extract the document technical image metadata from its IIIF manifest\n",
    "2. Load the IIIF images\n",
    "3. Apply a SSD Resnet model with openCV/dnn module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert here the Gallica document ID you want to process\n",
    "#docID = '12148/bpt6k46000341' # quotidien\n",
    "#docID = '12148/btv1b6931954n' # photo\n",
    "#docID = '12148/btv1b10336854c' # album\n",
    "docID = '12148/btv1b10544068q' # estampe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We ask for the document IIIF manifest to know more about its images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we build the IIIF URL\n",
    "import requests\n",
    "\n",
    "METADATA_BASEURL = 'https://gallica.bnf.fr/iiif/ark:/'\n",
    "req_url = \"\".join([METADATA_BASEURL, docID, '/manifest.json'])\n",
    "print (req_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we ask for the IIIF manifest. The call returns a dictionary\n",
    "r = requests.get(req_url)\n",
    "r.raise_for_status()\n",
    "json_4img = r.json()\n",
    "print (json_4img.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now we load the images files thanks to the IIIF Image protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iiif_api import IIIF #  get the image files with the IIIF Image API (PyGallica package again)\n",
    "\n",
    "# IIIF export factor (%)\n",
    "docExportFactor = 15\n",
    "\n",
    "# get the sequence of images metadata. It's a list\n",
    "sequences = json_4img.get('sequences')\n",
    "# get the canvases, first element of the list. Its a dict\n",
    "canvases = sequences[0]\n",
    "print (canvases.keys())\n",
    "# parse each canvas data for each image\n",
    "# each canvas has these keys: [u'height', u'width', u'@type', u'images', u'label', u'@id', u'thumbnail']\n",
    "nImages = 0\n",
    "urlsIIIF = []\n",
    "print (\"--- getting image metadata from the IIIF manifest...\")\n",
    "for c in canvases.get('canvases'): \n",
    "    nImages += 1\n",
    "    print (\" label:\",c.get('label'),\" width:\",c.get('width'), \" height:\",c.get('height'))\n",
    "    # we also get a Gallica thumbnail (it's not a IIIF image)\n",
    "    thumbnail = c.get('thumbnail')\n",
    "    urlThumbnail = thumbnail.get('@id')\n",
    "    #print \" thumbnail: \",urlThumbnail  \n",
    "    # we build the IIIF URL. We ask for the full image with a size factor of docExportFactor\n",
    "    urlIIIF = \"\".join([docID,'/f',str(nImages)]), 'full', \"\".join(['pct:',str(docExportFactor)]), '0', 'native', 'jpg'\n",
    "    urlsIIIF.append(urlIIIF)\n",
    "    #IIIF.iiif()\n",
    "     \n",
    "print (\"-------\")\n",
    "print (\"images:\", nImages)\n",
    "[IIIF.iiif(u[0],u[1],u[2],u[3],u[4],u[5]) for u in urlsIIIF]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We display the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image # pip install pillow\n",
    "from PIL.Image import Image as PilImage\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def path_to_pil(file):   \n",
    "    fileName = \"\".join([docID,\"/\",file]) # the images have been stored in a folder based on the document ID like 12148/btv1b103365619\n",
    "    #print \"--- loading image \",fileName,\"...\"\n",
    "    img = Image.open(fileName)\n",
    "    return img\n",
    "\n",
    "def display_images(\n",
    "    images, \n",
    "    columns=6, width=18, height=8, max_images=20, \n",
    "    label_wrap_length=20, label_font_size=8):\n",
    "\n",
    "    if len(images) == 1:\n",
    "        display(images[0])\n",
    "        return \n",
    "    if not images:\n",
    "        print (\"No images to display!\")\n",
    "        return\n",
    "\n",
    "    if len(images) > max_images:\n",
    "        print (\"Showing\", max_images, \" images of\", len(images))\n",
    "        images=images[0:max_images]\n",
    "\n",
    "    height = max(height, int(len(images)/columns) * height)\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        if hasattr(image, 'filename'):\n",
    "            title=image.filename\n",
    "            if title.endswith(\"/\"): title = title[0:-1]\n",
    "            title=os.path.basename(title)\n",
    "            title=textwrap.wrap(title, label_wrap_length)\n",
    "            title=\"\\n\".join(title)\n",
    "            plt.title(title, fontsize=label_font_size); \n",
    "            \n",
    "# first we read the images\n",
    "import os, fnmatch\n",
    "entries = fnmatch.filter(os.listdir(docID), '*.jpg')\n",
    "images = [path_to_pil(e) for e in entries]\n",
    "display_images(images)\n",
    "#for im in images:\n",
    "#   display(im)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now we process the images for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# using the dnn module : https://docs.opencv.org/master/d2/d58/tutorial_table_of_content_dnn.html\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import paths\n",
    "\n",
    "min_confidence = 0.25\n",
    "nbFaces = 0\n",
    "\n",
    "def process_image(im):\n",
    "\t# load the input image and construct an input blob for the image\n",
    "\t# by resizing to a fixed 300x300 pixels and then normalizing it\n",
    "\timage = cv2.imread(im.filename)\n",
    "\t(h, w) = image.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
    "\toutText=\"\"\n",
    "\tprint (\"**************\\n\",im.filename)\n",
    "\tdocID = im.filename[0:-4]\n",
    "    \n",
    "\tglobal nbFaces\n",
    "\t# pass the blob through the network and obtain the detections and predictions\n",
    "\tnet.setInput(blob)\n",
    "\tdetections = net.forward()\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t\t# extract the confidence (i.e., probability) associated with the prediction\n",
    "\t\t\tconfidence = detections[0, 0, i, 2]\n",
    "\t\t\t# filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "\t\t\tif (confidence > min_confidence):\n",
    "\t\t\t\t# compute the (x, y)-coordinates of the bounding box for the object\n",
    "\t\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t\t\tif ((endX>w) or (endY>h)):\n",
    "\t\t\t\t\tprint (\" out of image : %d %d\") % (w,h)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnbFaces += 1\n",
    "\t\t\t\t\ttext = \"{:.2f}%\".format(confidence * 100)\n",
    "\t\t\t\t\t#print \"\\t%s\" % text\n",
    "\t\t\t\t\t#print (startX, startY,(endX-startX),(endY-startY))\n",
    "\t\t\t\t\t# draw the boxes\n",
    "\t\t\t\t\tcv2.rectangle(image, (startX, startY), (endX, endY),(0, 0, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, text, (startX, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\t\t\t\t\t# build the csv data\n",
    "\t\t\t\t\tif (outText ==\"\"):\n",
    "\t\t\t\t\t\toutText = \" %d,%d,%d,%d,%.2f\" % (startX, startY,(endX-startX), (endY-startY), confidence)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\toutText = \"%s\\n %d,%d,%d,%d,%.2f\" % (outText, startX, startY,(endX-startX), (endY-startY),confidence)\n",
    "\n",
    "    # convert from openCV2 to PIL. Notice the COLOR_BGR2RGB which means that \n",
    "\t# the color is converted from BGR to RGB\n",
    "\tcolor_coverted = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tpil_image=Image.fromarray(color_coverted)\n",
    "        \n",
    "\tif outText != \"\":\n",
    "\t\tprint (outText)\n",
    "\t\treq_url = \"\".join([METADATA_BASEURL, docID, '/full'])\n",
    "\t\t#print req_url\n",
    "        # show the output image\n",
    "\t\t#cv2.imshow(\"Output\", image)\n",
    "\t\t#cv2.waitKey(0)     \n",
    "\telse:\n",
    "\t\tprint (\"\\tno detection!\")\n",
    "        \n",
    "\tdisplay(pil_image)\n",
    "        \n",
    "## Main ##       \n",
    "print(\" loading model...\")\n",
    "# Single Shot Detector (SSD) model / https://arxiv.org/abs/1512.02325\n",
    "# https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11\n",
    "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "[process_image(im) for im in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
